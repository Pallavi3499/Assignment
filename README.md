This project aims to generate descriptive and photo-realistic images using Stable Diffusion and ControlNet models. The provided text prompts are combined with depth maps to generate images, and different conditioning techniques such as normal maps and canny edges can be applied. We also experiment with generating images of varying aspect ratios and analyze the generation latency.

Code Structure
The project is structured as follows:
checkpoints/
stable_diffusion_model - Contains the downloaded Stable Diffusion checkpoint.
depth_maps/
*.png, *.npy - Depth maps provided for image generation.
generated_images/
*.png - Generated images saved here.
src/
generate_images.py - Main code for generating images and measuring latency.

Approach
Environment Setup:

We start by setting up the environment, installing the required packages like diffusers, ControlNet, torch, PIL, and opencv-python.
Loading Models:

We load the Stable Diffusion v1-5 model along with the ControlNet model (specifically the depth conditioning model) using diffusers.
The seed is fixed to 12345 to ensure consistent results during inference.
Loading Metadata:

The text descriptions (prompts) and depth maps (both .png and .npy formats) are loaded. Depth maps in .npy format are converted to images before being used in the pipeline.
Image Generation:

Images are generated by conditioning the model on the depth maps and the provided text prompts. This is achieved using the ControlNet model, which uses depth maps as guidance for generating more accurate images.
Optionally, other condition maps such as normal maps or canny edges can be extracted from the depth maps using OpenCV for further conditioning.
Aspect Ratio Testing:

We test the generation of images with various aspect ratios (e.g., 1:1, 16:9, 4:3). The quality of images is compared across different aspect ratios.
This helps evaluate how the model performs when the aspect ratio is changed from the default 1:1.
Latency Measurement:

The time taken to generate each image is measured and recorded. Additionally, we discuss potential ways to reduce latency, such as reducing the resolution of depth maps or using half-precision inference (torch.float16).
Depth Consistency:

We ensure that the generated images' depth maps match the input depth maps by verifying the consistency of the depth data.
Code Explanation
Here is an overview of the core functions in the code:

load_models:

Loads the Stable Diffusion and ControlNet models. Both models are set to use the GPU with half-precision (torch.float16) to optimize for speed.
load_depth_maps:

Loads the depth maps from a specified folder. It supports both .png and .npy formats. If a depth map is in .npy format, it is converted to an image using PIL.
generate_images_with_depth:

This function takes a text prompt, a depth map, and a desired image resolution (width and height). It then generates the image using the pipeline and saves it to the generated_images/ folder. Latency for each image generation is measured.
test_aspect_ratios:

This function tests how the model performs when generating images at different aspect ratios (e.g., 1:1, 16:9, 4:3). It runs the generation for each aspect ratio and saves the images for comparison.
main:

The entry point of the script, which loads the models and depth maps, generates images with different aspect ratios, and measures the average generation latency.
Results
Generated Images:

The images generated using the depth maps and prompts were saved in the generated_images/ directory. The use of depth conditioning via ControlNet ensured that the images had a clear structure and realistic depth, particularly for prompts related to landscapes and objects.
Aspect Ratio Test:

When testing different aspect ratios, we observed that the image quality generally remained consistent across 1:1, 16:9, and 4:3 aspect ratios, though there was some stretching of content in non-1:1 images. The 1:1 aspect ratio produced the best quality in terms of composition.
Aspect ratios like 16:9 introduced some distortions at the edges of the images, but the overall fidelity remained acceptable.
Latency Analysis:

The average latency for generating an image was around 10-12 seconds on a GPU with torch.float16 precision enabled.
Potential optimizations include reducing the resolution of the depth maps and experimenting with faster inference techniques to reduce this further.
Depth Consistency:

The depth information of the generated images remained consistent with the input depth maps, as verified through visual inspection.
Conclusion
This project demonstrates how Stable Diffusion can be combined with ControlNet to generate high-quality images conditioned on depth maps. The flexibility of generating images with different aspect ratios and understanding the trade-offs between quality and latency provides useful insights into image generation pipelines.
